{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ylCBTPmh4RC"
      },
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "\n",
        "> ## Introduction\n",
        "This lab focuses on using an algorithm based upon Neural Machine Translation to translate sentences from English to French.\n",
        "> ## Algorithm Description\n",
        "A Neural Machine Translation system is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x1, . . .xn, to a target sentence, y1, . . . , ym [[1](https://arxiv.org/abs/1508.04025)]. A basic form of NMT consists of two components:\n",
        "1. An encoder which computes a representation ***s*** for each source sentence. (We will use an RNN with GRU as hiddent unit.)\n",
        "2. A decoder which generates one target word at a time and hence de-composes the conditional probability as: $\\log p(y|x) =\\sum_{j=1}^{m} \\log p(y_j|y_{<j},s)$\n",
        " \n",
        "> The Decoder we will use is a non-stacked unidirectional RNN. It is a neural language model conditioned not only on the previously generated target words but also on the source sentence. More precisely, it generates the target sentence: $ y = (y 1 , . . . , y T_y)$ one word $y_t$ at a time based on the distribution: $ P[y_t |{y_1 , ..., y_{t−1}},c_t] = softmax(W_sh̃_t)$, where \n",
        "$h̃_t$ , the attentional hidden state, is computed as (biases are not shown for simplicity): $h̃_t = \\tanh( W_{c}[c_{t};h_t])$. $h_t$ is the $t^{th}$ hidden state of the decoder, $c_t$ is the source context vector, and $;$ denotes concatenation. $W_s$ and $W_c$ are matrices of trainable parameters.\n",
        "\n",
        "> **Note:** While all the inputs of the encoder (i.e., all the words of the input sentence) are known at encoding time, the decoder generates one target word at a time, and uses as input at time $t$ its prediction from time $t − 1$.\n",
        "> ## Global Attention Mechanism\n",
        "The context vector $c_t$ is computed as a weighted sum of the encoder’s hidden states $h̄_i$ . The vector of weights $\\alpha_t$ is obtained by applying a softmax to the output of an alignment operation (score())\n",
        "between the current target hidden state $h_t$ and all source hidden states $h̄_{i}$'s. $\\alpha_t$ indicates which words\n",
        "in the source sentence are the most likely to help in predicting the next word. *score()* can in theory\n",
        "be any comparison function. In our implementation, we will use the concat attention formulation of\n",
        "[[1 Section 3.1](https://arxiv.org/abs/1508.04025)]. An overview is provided in the following figure: \n",
        "\n",
        "\n",
        "![Figure-1](https://drive.google.com/uc?export=view&id=12duBYnzyMR06sMv2VfXZWbvK8fvJs7om)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9PcwitErp7A",
        "colab_type": "code",
        "outputId": "76402ad4-989f-42f7-aa68-5c92bbb7742c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "##### Mounting Drive for datasets #######\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-B_q4GadmL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####Creating Encoder Class#########\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        print('Start Encoder')\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        embedded_input = self.embedding(input) ## Output: (∗,H), where * is the input shape and H=embedding_dim \n",
        "        #Pass input of shape (seq_len, batch, input_size) to GRU\n",
        "        #Get 1. output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \n",
        "        #Get 2. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
        "        output_tensor, hs = self.rnn(embedded_input)\n",
        "        # you should return a tensor of shape (seq,batch,feat)\n",
        "        print('End Encoder')\n",
        "        return output_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zavE8nAKzTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####Creating Decoder Class#######\n",
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim,hidden_dim) # 60 , 30\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1,batch,vocab), h and tilde_h of shape (1,batch,feat)\n",
        "        print('Start decoder')\n",
        "        #print('Shape of input: ',input.size())\n",
        "        embedded_input=self.embedding(input)\n",
        "        rnn_output, decoder_hidden = self.rnn(embedded_input,h)\n",
        "        #print('Shape of rnn output: ',rnn_output.size())\n",
        "        #print('Shape of source_context: ',source_context.size())\n",
        "        #rnn_output = rnn_output.squeeze(0)\n",
        "        #source_context=source_context.squeeze(0)\n",
        "        #print('Shape of rnn_output after squeeze: ',rnn_output.size())\n",
        "        concat_input = torch.cat([source_context,decoder_hidden],dim=2)\n",
        "        #print('Shape after concatenating rnn_output with src_context: ',concat_input.size())\n",
        "        #print('Next 1')\n",
        "        output = torch.tanh(self.ff_concat(concat_input))\n",
        "        output = self.predict(output)\n",
        "        print('End Decoder')\n",
        "        return output, decoder_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zSXRlx_6X0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Creating Attention Class #########\n",
        " class seq2seqAtt(nn.Module):   \n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t,hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim,1,bias=False) # just a dot product here\n",
        "        self.va=nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "    \n",
        "    def forward(self,target_h,source_hs):\n",
        "        # Create a copy of target_h Tx times ( hidden_dim_s )\n",
        "        print('Start attention')\n",
        "        #print('Shape of source_hs: ', source_hs.size())\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0),1,1) # (1,batch,feat) -> (seq,batch,feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "       \n",
        "        # Concatenate target_h with each source hidden state (h of t')\n",
        "        concat_output =(self.ff_concat(torch.cat([target_h_rep,source_hs],dim=2))) \n",
        "        #print('Concat_ouput shape: ',concat_output.size())\n",
        "        scores = self.ff_score(torch.tanh(concat_output))  # should be of shape (seq,batch,1)\n",
        "        #print('Shape of scores: ',scores.size())\n",
        "        scores = scores.squeeze(dim=2) # (seq,batch,1) -> (seq,batch). dim=2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        #print('Shape of scores after squeeze: ',scores.size())\n",
        "        norm_scores = torch.softmax(scores,0) #alphas\n",
        "        # Dot product of context vector with source hidden state vector\n",
        "        source_hs_p = source_hs.permute((2,0,1)) # (seq,batch,feat) -> (feat,seq,batch)\n",
        "        #print('Shape of norm_scores: ',norm_scores.size())\n",
        "        #print('Shape of source_hs_p: ',source_hs_p.size())\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq,batch) * (feat,seq,batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1,2,0)),0,keepdim=True) # (feat,seq,batch) -> (seq,batch,feat) -> (1,batch,feat); keepdim otherwise sum squeezes \n",
        "        print('Done attention')\n",
        "        return ct,norm_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9TCQmO5YUGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######Defining Seq2SeqModel Class ########\n",
        "\n",
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "        \n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        #print('max source index',self.max_source_idx)\n",
        "        #print('source vocab size',len(vocab_s))\n",
        "        \n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        #print('max target index',self.max_target_idx)\n",
        "        #print('target vocab size',len(vocab_t_inv))\n",
        "        \n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.encoder = Encoder(self.max_source_idx+1,self.embedding_dim_s,self.hidden_dim_s,self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1,self.embedding_dim_t,self.hidden_dim_t,self.padding_token).to(self.device)\n",
        "        \n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att,self.hidden_dim_s,self.hidden_dim_t).to(self.device)\n",
        "    \n",
        "    def my_pad(self,my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1,tensor_t_1),...,(tensor_s_batch,tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0],torch.LongTensor([self.eos_token]))) for elt in my_list],batch_first=True,padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1],torch.LongTensor([self.eos_token]))) for elt in my_list],batch_first=True,padding_value=self.padding_token)\n",
        "        return batch_source,batch_target\n",
        "    \n",
        "    def forward(self,input,max_size,is_prod):\n",
        "        #print('In forward')\n",
        "        if is_prod: \n",
        "            input = input.unsqueeze(1) # (seq) -> (seq,1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        \n",
        "        current_batch_size = input.size(1)\n",
        "    \n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        # Arguments: vocab_size, embedding_dim, hidden_dim, padding_idx\n",
        "        #encoder = Encoder(len(self.vocab_s),self.embedding_dim_s,self.hidden_dim_s,None)\n",
        "        source_hs = self.encoder(input)\n",
        "        \n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        \n",
        "        target_h = torch.zeros(size=(1,current_batch_size,self.hidden_dim_t)).to(self.device) # init (1,batch,feat)\n",
        "        \n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1,batch)\n",
        "        print('Shape of target input:',target_input.shape)\n",
        "        \n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        att_list = []\n",
        "        while True:\n",
        "            \n",
        "            #print(counter)\n",
        "            #counter = counter + 1\n",
        "            if self.do_att:\n",
        "                source_context, attentions = self.att_mech(target_h,source_hs) # (1,batch,feat)\n",
        "            else:\n",
        "                source_context = source_hs[-1,:,:].unsqueeze(0) # (1,batch,feat) last hidden state of encoder\n",
        "            \n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            # Arguments: vocab_size, embedding_dim, hidden_dim, padding_idx)\n",
        "            #Decoder(len(self.vocab_t_inv),self.embedding_dim_t,self.hidden_dim_t,None)\n",
        "            prediction, target_h = self.decoder(target_input,source_context,target_h)\n",
        "            \n",
        "            logits.append(prediction) # (1,batch,vocab)\n",
        "            att_list.append(attentions.cpu().detach().numpy().squeeze())\n",
        "\n",
        "            \n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            #source_context = self.att_mech.forward(target_h,source_hs)\n",
        "            #topv, topi = prediction.topk(1)\n",
        "            target_input = torch.argmax(prediction,2)#topi.squeeze(-1).detach().unsqueeze(0)\n",
        "\n",
        "           \n",
        "            #target_input = target_input.unsqueeze(0)\n",
        "            #Variable(torch.LongTensor([self.SOS] * batch_size)).squeeze(-1)\n",
        "            \n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            \n",
        "            pos += 1\n",
        "            if pos>=max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        \n",
        "        to_return = torch.cat(logits,0) # logits is a list of tensors -> (seq,batch,vocab)\n",
        "        \n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq,vocab)\n",
        "        \n",
        "        return to_return\n",
        "    \n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        \n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        \n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        \n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        \n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch,seq)\n",
        "        \n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        \n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys,[0.0,0.0]))\n",
        "        \n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "        \n",
        "        for epoch in range(n_epochs):\n",
        "                   \n",
        "            with tqdm(total=len(train_loader),unit_scale=True,postfix={'loss':0.0,'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1),ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source,batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1,0).to(self.device) # RNN needs (seq,batch,feat) but loader returns (batch,seq)                        \n",
        "                        batch_target = batch_target.transpose(1,0).to(self.device) # (seq,batch)\n",
        "                        \n",
        "                        # are we using the model in production / as an API?\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq,batch), i.e., train or test\n",
        "                        \n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "                        print('Start fit')\n",
        "                        unnormalized_logits = self.forward(batch_source,max_size,is_prod)\n",
        "                        print('Shape of logits: ',unnormalized_logits.shape)\n",
        "                        print('End fit')\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1),batch_target.flatten())\n",
        "                  \n",
        "                        total_loss += sentence_loss.item()\n",
        "                        \n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
        "                        \n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        \n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "            \n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "            \n",
        "            if patience_counter>patience:\n",
        "                break\n",
        "    \n",
        "    def sourceNl_to_ints(self,source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean,self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "        \n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints \n",
        "    \n",
        "    def targetInts_to_nl(self,target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "    \n",
        "    def predict(self,source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits,attentions = self.forward(source_ints,self.max_size,True) # (seq) -> (<=max_size,vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size,1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl), attentions\n",
        "        \n",
        "    def save(self,path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs,path_to_file)\n",
        "    \n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls,path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new\n",
        "\n",
        "    def showAttention(self,input_sentence, output_words, attentions, path_to_output_images):\n",
        "       # Set up figure with colorbar\n",
        "        #figure(num=None, figsize=(20, 20), dpi=80, facecolor='w', edgecolor='k')\n",
        "        fig = plt.figure(figsize = (10,10))\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(np.array(attentions,dtype=np.float64), cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "        ax.set_yticklabels([''] + output_words )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.plot()\n",
        "        \n",
        "        fig.savefig(path_to_output_images + 'attention_evaluation.png')\n",
        "\n",
        "\n",
        "    def evaluateAndShowAttention(self,input_sentence, path_to_output_images):\n",
        "      output_words, attentions = self.predict(input_sentence)\n",
        "      # print('input =', input_sentence)\n",
        "      # print('output =', output_words)\n",
        "      output_words = output_words.split(' ')\n",
        "      for i,elt in enumerate(output_words):\n",
        "          # print(elt)\n",
        "          if elt=='.':\n",
        "              output_words = output_words[:i]\n",
        "              attentions = attentions[:i]\n",
        "              break\n",
        "      self.showAttention(input_sentence,output_words, attentions, path_to_output_images)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zk-kqRykbfH",
        "colab_type": "code",
        "outputId": "2581f8f9-eb58-4fda-d07a-05804f592736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "###Test Phase#####\n",
        "\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# = = = = = = = = = = =\n",
        "\n",
        "path_root = '/content/drive/My Drive/Colab Notebooks/AlteGrad2019/Lab4/'\n",
        "\n",
        "#path_to_model = path_root + '/code/'\n",
        "#sys.path.insert(0, path_to_model)\n",
        "\n",
        "#from model import seq2seqModel\n",
        "\n",
        "path_to_data = path_root + 'data/'\n",
        "path_to_save_models = path_root + 'models/'\n",
        "\n",
        "# = = = = = = = = = = =\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp\n",
        "\n",
        "\n",
        "# = = = = = = = = = = =\n",
        "\n",
        "do_att = True # should always be set to True\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "        \n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "    \n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "    \n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "    \n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "    \n",
        "    print('data loaded')\n",
        "        \n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "    \n",
        "    print('data prepared')\n",
        "    \n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "    \n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "    \n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')\n",
        "\n",
        "else:\n",
        "    \n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "    \n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt,model.predict(elt)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-05db2f270aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seqModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save_models\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'pretrained_moodle.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     to_test = ['I am a student.',\n",
            "\u001b[0;32m<ipython-input-7-3e9dc6767ef9>\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path_to_file)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# * list and ** names (dict) see args and kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for seq2seqModel:\n\tMissing key(s) in state_dict: \"att_mech.va\". "
          ]
        }
      ]
    }
  ]
}